{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6e0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader #shuffles\n",
    "from wilds.common.data_loaders import get_eval_loader #doesn't shuffle\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "183c407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset, and download it if necessary\n",
    "dataset = get_dataset(dataset=\"civilcomments\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e1736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0, 'val': 1, 'test': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.split_dict\n",
    "#get_subset(\"train/val/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fdab499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use loader to load validation set, as it's smaller than train set\n",
    "val_data = dataset.get_subset(\"val\")\n",
    "val_loader = get_eval_loader(\"standard\", val_data, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbfcc1",
   "metadata": {},
   "source": [
    "# Info on Wilds dataset\n",
    "\n",
    "size of validation set: 45,180 comments\n",
    "\n",
    "split into 4 groups\n",
    "1. train data\n",
    "2. test data\n",
    "3. Prod pre-shift data\n",
    "4. Prod post-shift data\n",
    "\n",
    "Metadata: The metadata contains information like the domain identity, e.g., which camera a photo was taken from, or which hospital the patient's data came from, etc., as well as other metadata.\n",
    "\n",
    "\"\"\"\n",
    "    The CivilComments-wilds toxicity classification dataset.\n",
    "    This is a modified version of the original CivilComments dataset.\n",
    "    Supported `split_scheme`:\n",
    "        'official'\n",
    "    Input (x):\n",
    "        A comment on an online article, comprising one or more sentences of text.\n",
    "    Label (y):\n",
    "        y is binary. It is 1 if the comment was been rated as toxic by a majority of the crowdworkers who saw that comment, and 0 otherwise.\n",
    "    Metadata:\n",
    "        Each comment is annotated with the following binary indicators:\n",
    "        Source: https://github.com/p-lambda/wilds/blob/a7a452c80cad311cf0aabfd59af8348cba1b9861/wilds/datasets/civilcomments_dataset.py#L9\n",
    "        \n",
    "            - male\n",
    "            - female\n",
    "            - LGBTQ\n",
    "            - christian\n",
    "            - muslim\n",
    "            - other_religions\n",
    "            - black\n",
    "            - white\n",
    "            - identity_any\n",
    "            \n",
    "            - severe_toxicity\n",
    "            - obscene\n",
    "            - threat\n",
    "            - insult\n",
    "            - identity_attack\n",
    "            - sexual_explicit\n",
    "            \n",
    "\"Next, for each category of demographics (e.g., race, gender), we construct an auxiliary\n",
    "    attribute (e.g., `na_race`, `na_gender`) that is 1 if the comment has no identities related to\n",
    "    that demographic, and is 0 otherwise.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "363b24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "y_train = []\n",
    "MAX = 1000 #number of texts to load\n",
    "\n",
    "#go through data and append to texts\n",
    "count = 0\n",
    "for batch in val_loader:\n",
    "    #print(len(batch)) #3 components\n",
    "    x, y, domain = batch\n",
    "    #print(x)\n",
    "    #print(type(x)) #tupe (text,)\n",
    "    \n",
    "    text = x[0]\n",
    "    texts.append(text)\n",
    "    y_train.append(y.numpy()[0]) #y is tensor object tensor([1])\n",
    "    #print(type(y.numpy()[0]))\n",
    "    \n",
    "    count +=1\n",
    "    if count == MAX:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0de7d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(texts), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "534fc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensor\n",
    "y_train = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90945289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Instantiate DistilBERT tokenizer, using FAST to optimize run-time\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe388e3",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "using bert tokenizer:\n",
    "\n",
    "inputs = tokenizer.tokenize(text)\n",
    "input_toks = tokenizer.encode(text)\n",
    "tokenizer.convert_ids_to_tokens(input_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55b19f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(texts)\n",
    "max_length = 264\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "    \n",
    "#tokenize the input texts \n",
    "batch = texts\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids.extend(inputs['input_ids'])\n",
    "attention_mask.extend(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4f218fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 264 <class 'list'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids), len(input_ids[0]), type(input_ids), type(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4fe44d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "#extract the tokenized input_ids and attention mask from tokenizer\n",
    "#this is a pytorch implementation with torch tensors\n",
    "#so we convert to numpy and back to tensorflow tensor\n",
    "X_train_ids = inputs['input_ids']\n",
    "X_train_attention = inputs['attention_mask']\n",
    "print(type(X_train_ids))\n",
    "\n",
    "X_train_ids = X_train_ids.numpy()\n",
    "X_train_attention = X_train_attention.numpy()\n",
    "\n",
    "print(type(X_train_ids))\n",
    "\n",
    "X_train_ids = tf.convert_to_tensor(X_train_ids)\n",
    "X_train_attention = tf.convert_to_tensor(X_train_attention)\n",
    "\n",
    "print(type(X_train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "20ca87e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f64df00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484293a1d76549ee8b4c93acce5eb26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 00:08:17.412688: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "DISTILBERT_DROPOUT = 0.2 #the dropout we add\n",
    "DISTILBERT_ATT_DROPOUT = 0.2\n",
    " \n",
    "# Configure DistilBERT's initialization\n",
    "config = DistilBertConfig(dropout=DISTILBERT_DROPOUT, \n",
    "                          attention_dropout=DISTILBERT_ATT_DROPOUT, \n",
    "                          output_hidden_states=True)\n",
    "                          \n",
    "# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
    "# and without any specific head on top.\n",
    "#this distilBERT is the transformer object \n",
    "#that is passed on to our build_model function at next set[]\n",
    "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Make DistilBERT layers untrainable\n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 264\n",
    "LAYER_DROPOUT = 0.2 \n",
    "LEARNING_RATE = 5e-5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def build_model(transformer, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # random seed\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE) \n",
    "    \n",
    "    # input layers with tokenized comments & attention\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                            name='input_ids', \n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='input_attention', \n",
    "                                                  dtype='int32')\n",
    "    \n",
    "\n",
    "    \n",
    "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
    "    \n",
    "\n",
    "    cls_token = last_hidden_state[:, 0, :]\n",
    "\n",
    "    \n",
    "    #custom output layer\n",
    "    output = tf.keras.layers.Dense(1, \n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_initializer=weight_initializer,  \n",
    "                                   kernel_constraint=None,\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(cls_token)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = build_model(distilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "30c628f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 264)]        0           []                               \n",
      "                                                                                                  \n",
      " input_attention (InputLayer)   [(None, 264)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  multiple            66362880    ['input_ids[0][0]',              \n",
      " BertModel)                                                       'input_attention[0][0]']        \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_distil_bert_model[4][7]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            769         ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 769\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bcc8ab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ids.shape\n",
    "type(X_train_ids)\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e4ccb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 561s 18s/step - loss: 0.7723 - accuracy: 0.3438\n"
     ]
    }
   ],
   "source": [
    "train_history1 = model_1.fit(x = [X_train_ids, X_train_attention],\n",
    "                          y = y_train,\n",
    "                          epochs = 1,\n",
    "                          batch_size = 32,\n",
    "                          steps_per_epoch = 1000 // 32,\n",
    "                          verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
